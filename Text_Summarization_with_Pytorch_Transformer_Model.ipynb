{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e25xHrO0LTXx"
      },
      "source": [
        "# Text Summarization with PyTouch Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q3HeKv4GSx-"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1wNsVOuGSx-"
      },
      "source": [
        "With the contentious evolution in digital age, individuals and organizations are often overwhelmed with vast amounts of textual data from various sources like news articles, research papers, and social media posts. This information overload poses a significant challenge as it is time-consuming and inefficient to manually read and understand all the available content. Moreover, the need for quick decision-making in various fields necessitates the availability of concise and relevant information. Therefore, there is a pressing need for an effective solution that can automatically condense large volumes of text into shorter summaries, preserving the key points and overall meaning. This solution should be capable of handling different types of text and should be scalable to accommodate the growing amount of digital content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfK6Ja2LGSx_"
      },
      "source": [
        "With Text summarization we can Improve:\n",
        "\n",
        "1. **Efficiency**: It helps in quickly understanding the main points of a large text without having to read the entire content. This saves time and effort, especially when dealing with lengthy documents or articles.\n",
        "\n",
        "2. **Information Overload**: With the vast amount of information available today, it's impossible to read and understand everything. Text summarization helps in managing this information overload by providing the key points in a concise manner.\n",
        "\n",
        "3. **Decision Making**: Summaries can aid in decision-making processes by providing the necessary information in a condensed form. This is particularly useful in business and research contexts where quick and informed decisions are crucial.\n",
        "\n",
        "4. **Accessibility**: Summarization makes information more accessible to people. For instance, news aggregator apps like Google News and Inshorts use text summarization to provide brief news items that users can quickly go through.\n",
        "\n",
        "5. **Search Optimization**: Summarization can improve search engine optimization by providing summaries of web pages, making it easier for users to find relevant information.\n",
        "\n",
        "In essence, text summarization plays a vital role in improving productivity, managing information overload, aiding decision-making, enhancing accessibility, and optimizing search results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cyeRjCoLTT3"
      },
      "source": [
        "## Introduced Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__YZfwlpLTRw"
      },
      "source": [
        "Text summarization in Natural Language Processing (NLP) is the process of condensing a large amount of text into a shorter, more digestible form, while preserving the key points and overall meaning. It involves breaking down lengthy text into smaller paragraphs or sentences, extracting vital information, and ensuring the meaning of the text is maintained.\n",
        "\n",
        "There are two main types of text summarization methods:\n",
        "\n",
        "1. **Extractive Text Summarization**: This is the traditional method where the significant sentences of the text are identified and added to the summary. The summary obtained contains exact sentences from the original textÂ¹.\n",
        "2. **Abstractive Text Summarization**: This is a more advanced method where the important sections are identified, the context is interpreted, and the information is reproduced in a new way. This ensures that the core information is conveyed through the shortest text possible. Here, the sentences in the summary are generated, not just extracted from the original text.\n",
        "\n",
        "As for its importance, text summarization is crucial because it improves productivity by saving time and providing concise information. It reduces the time required for understanding lengthy pieces such as articles without losing vital information. This is particularly useful in fields like news aggregation, research, legal, financial, and healthcare industries where large amounts of text need to be understood quickly and accurately. For instance, news aggregator apps like Google News and Inshorts take advantage of text summarization algorithms. It's like creating a quick version that highlights the main ideas, making it easier and faster for people to understand.\n",
        "\n",
        "The Transformer is one of the most powerful models in modern machine learning. They have revolutionized the field, particularly in Natural Language Processing (NLP) tasks such as language translation and text summarization. Long Short-Term Memory (LSTM) networks have been replaced by Transformers in these tasks due to their ability to handle long-range dependencies and parallel computations.\n",
        "\n",
        "The tool utilized in this notebook to build the Transformer is PyTorch, a popular open-source machine learning library known for its simplicity, versatility, and efficiency. With a dynamic computation graph and extensive libraries, PyTorch has become a go-to for researchers and developers in the realm of machine learning and artificial intelligence.\n",
        "\n",
        "Source:\n",
        "\n",
        "(1) Text Summarization Approaches for NLP - Machine Learning Plus. - https://www.machinelearningplus.com/nlp/text-summarization-approaches-nlp-example/.\n",
        "\n",
        "(2) Text Summarization in NLP - GeeksforGeeks. - https://www.geeksforgeeks.org/text-summarization-in-nlp/.\n",
        "\n",
        "(3) What is NLP Text Summarization: Benefits & Use Cases. https://www.accern.com/resources/what-is-nlp-text-summarization-benefits-use-cases.\n",
        "\n",
        "(4) Natural Language Processing for Text Summarization - NLP Stuff. https://nlpstuff.com/natural-language-processing-for-text-summarization/.\n",
        "\n",
        "(5) Text Summarization with Natural Language Processing (NLP). - https://www.analyticsvidhya.com/blog/2021/11/a-beginners-guide-to-understanding-text-summarization-with-nlp/.\n",
        "\n",
        "(6) Text Summarization | Papers With Code. - https://paperswithcode.com/task/text-summarization.\n",
        "\n",
        "(7) A Comprehensive Guide to Natural Language Processing Text Summarization - https://machinemindmatters.com/posts/nlp-text-summarization-guide/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMOPYoXMLTIQ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "In order to implement our text summerization model, we need a text data to work on it. for this task I have used [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews/code?datasetId=18&sortBy=voteCount)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PasxLv48LTF6"
      },
      "source": [
        "### Context\n",
        "This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n",
        "\n",
        "### Contents\n",
        "`Reviews.csv`: Pulled from the corresponding SQLite table named Reviews in database.sqlite\n",
        "\n",
        "`database.sqlite`: Contains the table 'Reviews'\n",
        "\n",
        "### Data includes:\n",
        "\n",
        "This dataset consists of half a million Amazon food reviews and was originally published by [SNAP](https://snap.stanford.edu/data/web-FineFoods.html).\n",
        "\n",
        "Some interesting avenues of exploration on it include:\n",
        "\n",
        "1. what does the product-reviewer graph look like?\n",
        "2. what words tend to indicate positive and negative reviews?\n",
        "3. what types of food products get reviewed the most?\n",
        "4. how does review score distribution vary across reviewers?\n",
        "5. what makes a review helpful?\n",
        "\n",
        "Reviews from Oct 1999 - Oct 2012\n",
        "\n",
        "568,454 reviews\n",
        "\n",
        "256,059 users\n",
        "\n",
        "74,258 products\n",
        "\n",
        "260 users with > 50 reviews\n",
        "\n",
        "This dataset is incredibly simple in structure, but that simple structure belies it's richness: it includes a complex graph of product-reviewer relations as well as a rich text dataset.\n",
        "\n",
        "\n",
        "### Acknowledgements\n",
        "If you publish articles based on this dataset, please cite the following paper:\n",
        "\n",
        "J. McAuley and J. Leskovec. [From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews](http://i.stanford.edu/~julian/pdfs/www13.pdf). WWW, 2013."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU4I_6uvLS5W"
      },
      "source": [
        "## Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYmdZgCKGSyB"
      },
      "source": [
        "I have build this entire notebook with `Python3` on google colabs with `T4 GPU` as my main hardware accelerator processing unit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0qYv5hIGSyC"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbRFpbB6GSyC"
      },
      "source": [
        "1. Utilizing GPU processors on Colab (Optional).\n",
        "2. Access Kaggle & Downloading Dataset remotely through kaggle API (Optional).\n",
        "3. Importing Librares.\n",
        "4. Loading & Explorating Dataset.\n",
        "5. Verify dataset.\n",
        "\n",
        "**Note**: I have set the GPU processing and Kaggle API access as optional steps since you can utilize a CPU and download data locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBmKiZWFGSyC"
      },
      "source": [
        "To change hardware accelerator to GPU on Colab with step by step guide, you can follow the following [link](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67qm9MTOOgXc",
        "outputId": "ec03ce2f-6c90-491b-92ca-9604c136f59e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Utilizing GPU Processors on Colab\n",
        "import tensorflow as tf\n",
        "# Gets the name of the GPU device if available\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# Checks if the GPU device name is available or not\n",
        "if device_name != '/device:GPU:0':\n",
        "  Print('GPU device not found')\n",
        "# If a GPU is found, its device name is printed.\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87NOTumDGSyE"
      },
      "source": [
        "The `nvidia-smi` command is a utility from NVIDIA that is used for the management and monitoring of NVIDIA GPU devices.\n",
        "\n",
        "The `!` symbol before the command indicates that it's being run in a shell environment.\n",
        "\n",
        "Here's a brief overview of what `nvidia-smi` does:\n",
        "\n",
        "- It provides information about the state of NVIDIA GPUs, including details like temperature, power usage, memory usage, and more.\n",
        "- It allows administrators to query GPU device state and, with the appropriate privileges, permits administrators to modify GPU device state.\n",
        "- It can report query information as XML or human-readable plain text to either standard output or a file.\n",
        "- It is targeted at the Teslaâ¢, GRIDâ¢, Quadroâ¢, and Titan X products, though limited support is also available on other NVIDIA GPUs.\n",
        "- It is installed along with the CUDA toolkit and provides meaningful insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkrjZ12rzP2J",
        "outputId": "0cf94e31-a26e-4be8-c3d7-58d8fca40358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 17 08:10:17 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0              27W /  70W |    103MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# GPU State Information\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aigj8FpuGSyE"
      },
      "source": [
        "Understanding some of the details in the output of `nvidia-smi`:\n",
        "\n",
        "- **Temp**: Core GPU temperature in degrees Celsius.\n",
        "- **Perf**: Denotes GPU's current performance state. It ranges from P0 to P12 referring to maximum and minimum performance respectively.\n",
        "- **Persistence-M**: The value of Persistence Mode flag where \"On\" means that the NVIDIA driver will remain loaded (persist) even when no active client such as Nvidia-smi is running.\n",
        "- **Pwr: Usage/Cap**: It refers to the GPU's current power usage out of total power capacity. It samples in Watts.\n",
        "- **Memory-Usage**: Denotes the memory allocation on GPU out of total memory.\n",
        "\n",
        "Please note that the actual output and available features may vary depending on the specific GPU model and the version of the NVIDIA driver installed on your system. For more details, you can refer to the `nvidia-smi` documentation.\n",
        "\n",
        "Source:\n",
        "\n",
        "(1) System Management Interface SMI | NVIDIA Developer. https://developer.nvidia.com/nvidia-system-management-interface.\n",
        "\n",
        "(2) Explained Output of Nvidia-smi Utility | by Shachi Kaul - Medium. https://medium.com/analytics-vidhya/explained-output-of-nvidia-smi-utility-fc4fbee3b124.\n",
        "\n",
        "(3) How to use nvidia-smi command on windows and ubuntu linux. https://www.gpu-mart.com/blog/monitor-gpu-utilization-with-nvidia-smi.\n",
        "\n",
        "(4) How to use the command 'nvidia-smi' (with examples). https://commandmasters.com/commands/nvidia-smi-common/.\n",
        "\n",
        "(5) Get Information About Nvidia GPU using nvidia-smi | Lindevs. https://lindevs.com/get-information-about-nvidia-gpu-using-nvidia-smi.\n",
        "\n",
        "(6) undefined. https://www.nvidia.com/en-us/about-nvidia/partners/.\n",
        "\n",
        "(7) Getty Images. https://www.gettyimages.com/detail/news-photo/an-exterior-view-of-the-nvidia-headquarters-on-may-30-2023-news-photo/1494623399."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FvcM_6WGSyF"
      },
      "source": [
        "for working with CPU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG-yMr0KGSyF"
      },
      "source": [
        "This part of code helps to know how many cores can be used on training and if can be utilized in multiprocessing. as well as get to know how much it may take to train your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j49VK87IzvGa",
        "outputId": "2011c83c-0c96-4078-a2d7-27e55dfb85d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CPU cores: 2\n"
          ]
        }
      ],
      "source": [
        "#  Allows creation of process-based parallel programs.\n",
        "import multiprocessing\n",
        "# Get the number of CPU cores on the current machine\n",
        "# The number of cores is then stored in the variable\n",
        "cores = multiprocessing.cpu_count()\n",
        "#  Check the number of CPU cores\n",
        "print(f\"Number of CPU cores: {cores}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmApDn_4GSyF"
      },
      "source": [
        "Install Kaggle Packages:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_c5ERvLGSyF"
      },
      "source": [
        "To access kaggle API and create access token follow this [guide](https://wellsr.com/python/how-to-import-kaggle-datasets-into-google-colab-using-google-drive/#:~:text=Create%20a%20Kaggle%20account%20and%20generate%20an%20API,Notebook.%20Load%20the%20datasets%20into%20your%20Colab%20notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BPa62xjwAQq5"
      },
      "outputs": [],
      "source": [
        "# Install Kaggle Package\n",
        "# we add ` &> /dev/null ` to silence its output outside of any errors that may arise.\n",
        "!pip install kaggle &> /dev/null\n",
        "!pip install datasist &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eTnLRpMLAX8j"
      },
      "outputs": [],
      "source": [
        "# create the .kaggle directory and an empty kaggle.json file\n",
        "!mkdir -p /root/.kaggle\n",
        "!touch /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Pc81eO-LGSyG"
      },
      "outputs": [],
      "source": [
        "# Fill in your user name and key from creating the kaggle account and API token file\n",
        "# I removed my own tokens for security perposes\n",
        "import json\n",
        "kaggle_username = \"Your_Username\" # \"Your_Username\"\n",
        "kaggle_key = \"you_Kaggle_API_Token\" # \"you_Kaggle_API_Token\"\n",
        "\n",
        "# Save API token the kaggle.json file\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
        "    f.write(json.dumps({\"username\": kaggle_username, \"key\": kaggle_key}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "upxEuhSRGSyG",
        "outputId": "bfbb8b85-7b64-427a-8dac-54b47de87d74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amazon-fine-food-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset, it will be in a .zip file so you'll need to unzip it as well.\n",
        "!kaggle datasets download -d snap/amazon-fine-food-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EwU9yJsJGSyG",
        "outputId": "39315ec9-0cd5-4048-b63e-fe3e62610054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  amazon-fine-food-reviews.zip\n",
            "  inflating: Reviews.csv             \n",
            "  inflating: database.sqlite         \n",
            "  inflating: hashes.txt              \n"
          ]
        }
      ],
      "source": [
        "# If you already downloaded it you can use the -o command to overwrite the file\n",
        "!unzip -o amazon-fine-food-reviews.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Mrp3t0g-G14s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M3ejUF3GGSyH"
      },
      "outputs": [],
      "source": [
        "# Import splited train and test subsets lib\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYgRjKycGSyG"
      },
      "source": [
        "For this project, I have used `d2l` package is a collection of Python scripts for the `âDive into Deep Learningâ` book, which is a comprehensive resource on deep learning. By installing `d2l`, I have access to all the necessary libraries and modules to run the codes from the book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y3rg9_8AGSyG"
      },
      "outputs": [],
      "source": [
        "# Will need d2l lib to perform a residual connection followed by layer normalization by using the Addnorm method\n",
        "# without importing this lib or it's replacement the encoder and decoder class funtion will give an error\n",
        "!pip install d2l &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pHowYGXGSyH"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rYrYETAQ90ox"
      },
      "outputs": [],
      "source": [
        "# Importing files module from google colab to handle file operations\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np  # Importing numpy library as np, which is useful for many scientific computing in Python\n",
        "import pandas as pd  # Importing pandas as pd, which is useful for data manipulation and analysis\n",
        "\n",
        "import nltk  # Importing nltk (Natural Language Toolkit), a library for symbolic and statistical natural language processing\n",
        "from nltk.corpus import stopwords  # Importing stopwords from nltk.corpus, a list of common words to be filtered out\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize  # Importing word_tokenize and sent_tokenize from nltk.tokenize to split text into words and sentences\n",
        "\n",
        "import collections  # Importing collections module which implements specialized container datatypes providing alternatives to Pythonâs general purpose built-in containers\n",
        "\n",
        "from collections import Counter  # Importing Counter from collections, a dict subclass for counting hashable objects\n",
        "\n",
        "import torch  # Importing PyTorch library, a popular open-source machine learning library\n",
        "from torch import nn  # Importing nn module from PyTorch, which contains neural network layers and functions\n",
        "from torch.utils.data import Dataset, DataLoader  # Importing Dataset and DataLoader from torch.utils.data, which provide utility functions to load and preprocess data\n",
        "\n",
        "from bs4 import BeautifulSoup  # Importing BeautifulSoup from bs4, a Python library for pulling data out of HTML and XML files\n",
        "\n",
        "import math  # Importing math module that provides mathematical functions\n",
        "\n",
        "import matplotlib.pyplot as plt  # Importing pyplot from matplotlib as plt, a plotting library used for 2D graphics\n",
        "import seaborn as sns  # Importing seaborn as sns, a Python data visualization library based on matplotlib\n",
        "\n",
        "from d2l import torch as d2l  # Importing torch module from d2l (Dive into Deep Learning), a library for deep learning in Python\n",
        "import os  # Importing os module that provides functions for interacting with the operating system\n",
        "import re  # Importing re module that provides support for regular expressions in Python\n",
        "\n",
        "import warnings  # Importing warnings module to handle warning messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QUBLe7rIqIIT"
      },
      "outputs": [],
      "source": [
        " # Using filterwarnings function from warnings module to ignore all warning messages\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Set pandas to display the entire content of a column without truncating\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rpnXRe2e3S8n"
      },
      "outputs": [],
      "source": [
        "# Disable multiprocessing in TensorFlow\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vaEQioyr3Asw"
      },
      "outputs": [],
      "source": [
        "# Disable multiprocessing in PyTorch\n",
        "torch.set_num_threads(1)\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VsHe5ZZHLo_",
        "outputId": "56dcf0a6-f406-4aa1-de19-0bff068f5d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Downloads the stopwords list for English\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RB2Svj2kQul3"
      },
      "outputs": [],
      "source": [
        "# Count the number of GPU devices available\n",
        "def get_device(i=0):\n",
        "    if torch.cuda.device_count() >= i+1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "device = get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWl1SCNuxm4L",
        "outputId": "de54cb02-e0a0-4bcd-9bcb-4f32358f2fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available? True\n"
          ]
        }
      ],
      "source": [
        "# Checks if there is GPU available to be used\n",
        "print(f\"Is CUDA available? {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5_ZMsV4GSyI"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZUl_xpkKAKMM"
      },
      "outputs": [],
      "source": [
        "# Load the dataset as a Pandas dataframe and store it in df\n",
        "df = pd.read_csv('Reviews.csv', nrows = 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7-jh7Fh3Bckq"
      },
      "outputs": [],
      "source": [
        "# Include only Text data with its Summary\n",
        "data = df[['Text', 'Summary']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OsPHxPOQBf96"
      },
      "outputs": [],
      "source": [
        "# Drop duplicate rows\n",
        "data.drop_duplicates(subset=['Text'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "T-2Dawh2FhKi",
        "outputId": "a0f44d8c-d788-4e5c-e74f-3a5e0de2ee23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Text  \\\n",
              "0                                                                                                                                                                                                                                                        I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.   \n",
              "1                                                                                                                                                                                                                                                                                                                                 Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".   \n",
              "2  This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.   \n",
              "3                                                                                                                                                                                                                                                                                                    If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                   Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.   \n",
              "\n",
              "                 Summary  \n",
              "0  Good Quality Dog Food  \n",
              "1      Not as Advertised  \n",
              "2  \"Delight\" says it all  \n",
              "3         Cough Medicine  \n",
              "4            Great taffy  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83bedb20-1928-4778-a439-0128e74f83e4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".</td>\n",
              "      <td>Not as Advertised</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.</td>\n",
              "      <td>Cough Medicine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.</td>\n",
              "      <td>Great taffy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83bedb20-1928-4778-a439-0128e74f83e4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-83bedb20-1928-4778-a439-0128e74f83e4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-83bedb20-1928-4778-a439-0128e74f83e4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-10f6a73a-baeb-494c-b2e2-ee488ba86405\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-10f6a73a-baeb-494c-b2e2-ee488ba86405')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-10f6a73a-baeb-494c-b2e2-ee488ba86405 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 9513,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9513,\n        \"samples\": [\n          \"Handy cups were easy to fill using the enclosed stand and performed quite well. Lids fit snugly and ran well through my Keurig machine.\",\n          \"I have a 11 yr old Pomimo (1/2 Pomeranian, 1/2 American Eskimo) who seems to only like human food. Although I'd love to give him human food all the time, I know it's not good for his digestive system, and it doesn't have the proper nutrients required for a dog, especially at his current age. Upon trying various brands including Blue and Call of the Wild, I couldn't justify the price in comparison to how little my dog liked the taste. After reading some positive reviews here on Amazon for Newman's, I decided to give it a go. Surprisingly, my dog started eating the food immediately after I poured some into his dish. He never does that with other dry dog food, and usually waits to eat after I've given him a nibble of everything that I eat during the day. What makes it even better is the reasonable price for the quality. Yes, I understand that organic and quality US-made food are naturally more expensive, however some of the brands charge double of Newman's and provide only half the taste. As humans, we're usually sold by the nutritious & wholesome advertisements when buying dog food, however I think its pretty important for my dog to actually like the taste of the food also. Newman's seems to be the ticket for both nutrition and taste. Will be buying Newman's as a regular!\",\n          \"I thought I would like this more than I do. Maybe it's my palate, but my first taste impression was pineapple, not orange or tangerine. The small portion size may be acceptable for an energy drink, but is not what I would call thirst-quenching. Not a good choice for anyone trying to control calories or carbs. On the plus side, it is sweet, but not too sweet and the level of carbonation is well-balanced.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8521,\n        \"samples\": [\n          \"Great, considering I usually prefer a light/medium roast\",\n          \"K cups disfigured and unusable\",\n          \"Small Kernals\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Read 1st five rows in data\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Qr31nv9OBhZi"
      },
      "outputs": [],
      "source": [
        "# This dictionary maps informal contractions and abbreviations to their formal equivalents.\n",
        "# It can be used to normalize text data by replacing informal language with its formal counterparts.\n",
        "word_mapping = {\"ain't\": \"is not\",\"aint\": \"is not\", \"aren't\": \"are not\",\"arent\": \"are not\",\n",
        "                \"can't\": \"cannot\",\"cant\": \"cannot\", \"'cause\": \"because\", \"cause\": \"because\",\n",
        "                \"could've\": \"could have\", \"couldn't\": \"could not\",\"didn't\": \"did not\",\n",
        "                \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
        "                \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\",\n",
        "                \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
        "                \"how's\": \"how is\",\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
        "                \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                \"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\n",
        "                \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "                \"it's\": \"it is\",\"let's\": \"let us\", \"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
        "                \"might've\": \"might have\", \"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                \"must've\": \"must have\", 'mstake':\"mistake\", \"mustn't\": \"must not\",\n",
        "                \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n",
        "                \"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n",
        "                \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n",
        "                \"she'd\": \"she would\", \"she'd've\": \"she would have\",\"she'll\": \"she will\",\n",
        "                \"she'll've\": \"she will have\", \"she's\": \"she is\",\"should've\": \"should have\",\n",
        "                \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                \"so's\": \"so as\",\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
        "                \"that's\": \"that is\",\"there'd\": \"there would\",\"there'd've\": \"there would have\",\n",
        "                \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
        "                \"they'd've\": \"they would have\",\"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
        "                \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                \"wasn't\": \"was not\",'wasnt':\"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
        "                \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
        "                \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
        "                \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
        "                \"where'd\": \"where did\", \"where's\": \"where is\",\"where've\": \"where have\",\n",
        "                \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n",
        "                \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                \"won't\": \"will not\", \"won't've\": \"will not have\",\"would've\": \"would have\",\n",
        "                \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
        "                \"you'll\": \"you will\", \"you'll've\": \"you will have\",\"you're\": \"you are\",\n",
        "                \"you've\": \"you have\", 'youve':\"you have\", 'goin':\"going\", '4ward':\"forward\",\n",
        "                \"shant\":\"shall not\",'tat':\"that\", 'u':\"you\", 'v': \"we\",'b4':'before', \"sayin'\":\"saying\"\n",
        "                }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VbpTJwIJBhTK"
      },
      "outputs": [],
      "source": [
        "# Data Cleaning funtion\n",
        "def text_cleaner(text):\n",
        "    \"\"\"\n",
        "    Cleans text data by:\n",
        "    - Lowercasing all characters\n",
        "    - Removing URLs and special characters\n",
        "    - Removing punctuation\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([word_mapping[t] if t in word_mapping else t for t in newString.split(\" \")])\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    long_words=[]\n",
        "\n",
        "    tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>=3:                  #removing short word\n",
        "            long_words.append(i)\n",
        "    text = \" \".join(long_words).strip()\n",
        "    def no_space(word, prev_word):\n",
        "        return word in set(',!\"\";.''?') and prev_word!=\" \"\n",
        "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
        "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]\n",
        "    text = ''.join(out)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "16RE_v0RJn9K"
      },
      "outputs": [],
      "source": [
        "# Tokenize function\n",
        "def tokenize(lines, token='word'):\n",
        "    assert token in ('word', 'char'), 'Unknown token type: ' + token\n",
        "    return [line.split() if token == 'word' else list(line) for line in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eef0hD-3Jn6H"
      },
      "outputs": [],
      "source": [
        "# Pading function\n",
        "def truncate_pad(line, num_steps, padding_token):\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps]  # Truncate\n",
        "    return line + [padding_token] * (num_steps - len(line))  # Pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XuZzX5QBJn3i"
      },
      "outputs": [],
      "source": [
        "# Vocabulary class\n",
        "class Vocab:\n",
        "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "        # Flatten a 2D list if needed\n",
        "        if tokens and isinstance(tokens[0], list):\n",
        "            tokens = [token for line in tokens for token in line]\n",
        "        # Count token frequencies\n",
        "        counter = collections.Counter(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "        # The list of unique tokens\n",
        "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "            return [self.idx_to_token[int(index)] for index in indices]\n",
        "        return self.idx_to_token[indices]\n",
        "\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return self.token_to_idx['<unk>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_8XsA1zPJn0q"
      },
      "outputs": [],
      "source": [
        "# Fun to add eos and padding and also determine valid length of each data sample\n",
        "def build_array_sum(lines, vocab, num_steps):\n",
        "    lines = [vocab[l] for l in lines]\n",
        "    lines = [l + [vocab['<eos>']] for l in lines]\n",
        "    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
        "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
        "    return array, valid_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pKu1ANVGJnyH"
      },
      "outputs": [],
      "source": [
        "# Create the tensor dataset object\n",
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2SdNOBrWJnvg"
      },
      "outputs": [],
      "source": [
        "# The main class\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = DotProductAttention(dropout)\n",
        "        self.w_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
        "        self.w_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
        "        self.w_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
        "        self.w_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries = transpose_qkv(self.w_q(queries), self.num_heads)\n",
        "        keys = transpose_qkv(self.w_k(keys), self.num_heads)\n",
        "        values = transpose_qkv(self.w_v(values), self.num_heads)\n",
        "        if valid_lens is not None:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, repeats = self.num_heads, dim=0)\n",
        "        output = self.attention(queries, keys, values, valid_lens)\n",
        "        output_concat = transpose_output(output, self.num_heads)\n",
        "        return self.w_o(output_concat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cTAxtIMeJnsU"
      },
      "outputs": [],
      "source": [
        "# Function to transpose the linearly transformed query key and values\n",
        "def transpose_qkv(X, num_heads):\n",
        "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(-1, X.shape[2], X.shape[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TWkNJJQgNYAn"
      },
      "outputs": [],
      "source": [
        "# For output formatting\n",
        "def transpose_output(X, num_heads):\n",
        "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(X.shape[0], X.shape[1], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wXN3e2_ZNX7J"
      },
      "outputs": [],
      "source": [
        "# Here masking is used so that irrelevant padding tokens are not considered\n",
        "# while calculations\n",
        "def sequence_mask(X, valid_len, value=0):\n",
        "    maxlen = X.size(1)\n",
        "    mask = torch.arange((maxlen), dtype=torch.float32, device=device)[None, :] < valid_len[:, None]    #device=X.device\n",
        "    X[~mask] = value\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "YMc8Xm-sNX4D"
      },
      "outputs": [],
      "source": [
        "# The irrelevant tokens are given a very small negative value which gets\n",
        "# Ignored in the subsequent calculations\n",
        "def masked_softmax(X, valid_lens):\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "98DGBTOeNX9-"
      },
      "outputs": [],
      "source": [
        "# The dot product attention scoring function\n",
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        d = queries.shape[-1]\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2))/math.sqrt(d)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "53QVA5jRNX1u"
      },
      "outputs": [],
      "source": [
        "# This line defines a class named `PositionWiseFFN` that inherits from `nn.Module`.\n",
        "class PositionWiseFFN(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a position-wise feed-forward network (FFN) layer commonly used in Transformer architectures.\n",
        "    It takes an input sequence, projects it to a higher dimensional space, applies a non-linear activation (ReLU),\n",
        "    and then projects it back to the desired output dimension. This allows the network\n",
        "    to learn complex non-linear relationships between elements in the sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_output, **kwargs):\n",
        "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
        "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_output)\n",
        "    def forward(self, X):\n",
        "        return self.dense2(self.relu(self.dense1(X)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8VUK0YVqNXyv"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  \"\"\"Implements positional encoding layer for Transformer models.\n",
        "\n",
        "  This layer adds positional information to an input embedding vector,\n",
        "  allowing the model to learn the relative position of words in a sequence.\n",
        "\n",
        "  Args:\n",
        "      num_hiddens (int): The number of features in the embedding vector.\n",
        "      dropout (float): Dropout rate to apply to the positional encoding.\n",
        "      max_len (int, optional): The maximum sequence length to support. Defaults to 1000.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_hiddens, dropout, max_len=1000):\n",
        "    \"\"\"\n",
        "    Initializes the positional encoding layer.\n",
        "\n",
        "    Args:\n",
        "        num_hiddens (int): Same as in the docstring.\n",
        "        dropout (float): Same as in the docstring.\n",
        "        max_len (int): Same as in the docstring.\n",
        "    \"\"\"\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.P = torch.zeros((1, max_len, num_hiddens))  # Positional encoding matrix\n",
        "\n",
        "    # Create positional encoding based on sine and cosine functions\n",
        "    X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
        "    self.P[:, :, 0::2] = torch.sin(X)  # Even dimension: sin\n",
        "    self.P[:, :, 1::2] = torch.cos(X)  # Odd dimension: cos\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Applies positional encoding to the input tensor.\n",
        "\n",
        "    Args:\n",
        "        X (torch.Tensor): Input tensor containing word embeddings.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The input tensor with positional encoding added.\n",
        "    \"\"\"\n",
        "    # Add positional encoding to input (relevant portion only)\n",
        "    X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
        "    return self.dropout(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "emuUbzi7NXv3"
      },
      "outputs": [],
      "source": [
        "# class for the block structure within\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input,\n",
        "                 ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
        "        super(EncoderBlock, self).__init__(**kwargs)\n",
        "        self.attention = MultiHeadAttention(key_size, query_size, value_size, num_hiddens,num_heads, dropout, use_bias)\n",
        "        self.addnorm1 = d2l.AddNorm(norm_shape, dropout)\n",
        "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
        "        self.addnorm2 = d2l.AddNorm(norm_shape, dropout)\n",
        "\n",
        "    def forward(self, X, valid_lens):\n",
        "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
        "        return self.addnorm2(Y, self.ffn(Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5AisZv7ZNXsf"
      },
      "outputs": [],
      "source": [
        "# the main encoder class\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
        "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_layers):\n",
        "            self.blks.add_module(\"block\"+str(i),EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias))\n",
        "\n",
        "    def forward(self, X, valid_lens, *args):\n",
        "        X = self.pos_encoding(self.embedding(X)*math.sqrt(self.num_hiddens))\n",
        "        self.attention_weights = [None]*len(self.blks)\n",
        "        for i, blk in enumerate(self.blks):\n",
        "            X = blk(X, valid_lens)\n",
        "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "c3CnC-i8OgjM"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape,\n",
        "                 ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs):\n",
        "        super(DecoderBlock, self).__init__(**kwargs)\n",
        "        self.i = i\n",
        "        self.attention1 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
        "        self.addnorm1 = d2l.AddNorm(norm_shape, dropout)\n",
        "        self.attention2 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
        "        self.addnorm2 = d2l.AddNorm(norm_shape, dropout)\n",
        "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
        "        self.addnorm3 = d2l.AddNorm(norm_shape, dropout)\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
        "        if state[2][self.i] is None: # true when training the model\n",
        "            key_values = X\n",
        "        else:                        # while decoding state[2][self.i] is decoded output of the ith block till the present time-step\n",
        "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
        "        state[2][self.i] = key_values\n",
        "        if self.training:\n",
        "            batch_size, num_steps, _ = X.shape\n",
        "            dec_valid_lens = torch.arange(1, num_steps+1, device = X.device).repeat(batch_size, 1)\n",
        "        else:\n",
        "            dec_valid_lens = None\n",
        "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
        "        Y = self.addnorm1(X, X2)\n",
        "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
        "        Z = self.addnorm2(Y, Y2)\n",
        "        return self.addnorm3(Z, self.ffn(Z)), state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3d1gVzCcOgeQ"
      },
      "outputs": [],
      "source": [
        "# The main decoder class\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens,\n",
        "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
        "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_layers):\n",
        "            self.blks.add_module(\"block\"+str(i),\n",
        "                                DecoderBlock(key_size, query_size, value_size,\n",
        "                                             num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i))\n",
        "            self.dense = nn.Linear(num_hiddens, vocab_size)\n",
        "\n",
        "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
        "        return [enc_outputs, enc_valid_lens, [None]*self.num_layers]\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        X = self.pos_encoding(self.embedding(X)*math.sqrt(self.num_hiddens))\n",
        "        self._attention_weights = [[None]*len(self.blks) for _ in range(2)]\n",
        "        for i, blk in enumerate(self.blks):\n",
        "            X, state = blk(X, state)\n",
        "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
        "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
        "        return self.dense(X), state\n",
        "\n",
        "    def attention_weights(self):\n",
        "        return self._attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2t7z__MHOgaX"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, enc_X, dec_X, *args):\n",
        "        enc_all_outputs = self.encoder(enc_X, *args)\n",
        "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
        "        # Return decoder output only\n",
        "        return self.decoder(dec_X, dec_state)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zVOwRULFOgUm"
      },
      "outputs": [],
      "source": [
        "def grad_clipping(net, theta):\n",
        "    if isinstance(net, nn.Module):\n",
        "        params = [p for p in net.parameters() if p.requires_grad]\n",
        "    else:\n",
        "        params = net.params\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "G3Upgzo0OgSM"
      },
      "outputs": [],
      "source": [
        "class Accumulator:\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MwNOTv12OgPW"
      },
      "outputs": [],
      "source": [
        "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
        "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "    # `label` shape: (`batch_size`, `num_steps`)\n",
        "    # `valid_len` shape: (`batch_size`,)\n",
        "    def forward(self, pred, label, valid_len):\n",
        "        weights = torch.ones_like(label)\n",
        "        weights = sequence_mask(weights, valid_len)\n",
        "        self.reduction='none'\n",
        "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
        "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
        "        return weighted_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "GVNM8DvqOgMQ"
      },
      "outputs": [],
      "source": [
        "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
        "    net.to(device)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    loss = MaskedSoftmaxCELoss()\n",
        "    net.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "        for batch in data_iter:\n",
        "            optimizer.zero_grad()\n",
        "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
        "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],device=device).reshape(-1, 1)\n",
        "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
        "            Y_hat = net(X, dec_input, X_valid_len)\n",
        "            l = loss(Y_hat, Y, Y_valid_len)\n",
        "            l.sum().backward()  # Make the loss scalar for `backward`\n",
        "            grad_clipping(net, 1)\n",
        "            num_tokens = Y_valid_len.sum()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                metric.add(l.sum(), num_tokens)\n",
        "        print(f\"Done with epoch number: {epoch+1}\") # optional step\n",
        "    print(f'loss {metric[0] / metric[1]:.3f} on {str(device)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "TeeUsPTWJnpc"
      },
      "outputs": [],
      "source": [
        "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,device, save_attention_weights=False):\n",
        "    # Set `net` to eval mode for inference\n",
        "    net.eval()\n",
        "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
        "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
        "    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
        "    # Unsqueeze adds another dimension that works as the the batch axis here\n",
        "    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
        "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
        "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
        "    # Add the batch axis to the decoder now\n",
        "    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
        "    output_seq, attention_weight_seq = [], []\n",
        "    for _ in range(num_steps):\n",
        "        Y = net.decoder(dec_X, dec_state)[0]\n",
        "        # We use the token with the highest prediction likelihood as the input\n",
        "        # of the decoder at the next time step\n",
        "        dec_X = Y.argmax(dim=2)\n",
        "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
        "        # Save attention weights\n",
        "        if save_attention_weights:\n",
        "            attention_weight_seq.append(net.decoder.attention_weights)\n",
        "            # Once the end-of-sequence token is predicted, the generation of the output sequence is complete\n",
        "        if pred == tgt_vocab['<eos>']:\n",
        "                break\n",
        "        output_seq.append(pred)\n",
        "    if len(output_seq)<2:\n",
        "\n",
        "        if len(output_seq)==1:\n",
        "            return ''.join(tgt_vocab.to_tokens(output_seq[0])), attention_weight_seq\n",
        "        else:\n",
        "\n",
        "            return \"No output!\", attention_weight_seq\n",
        "    else:\n",
        "        return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "awYeFnAKQBdD"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWIlO79DkPzJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hgHm0ULaBhRL"
      },
      "outputs": [],
      "source": [
        "data['cleaned_text'] = data['Text'].apply(text_cleaner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "qNfG3ViLBhOc"
      },
      "outputs": [],
      "source": [
        "data['cleaned_summary'] = data['Summary'].apply(text_cleaner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_3lKKZbeBhLP"
      },
      "outputs": [],
      "source": [
        "# this step is to remove all rows that have a blank summary\n",
        "data[\"cleaned_summary\"].replace('', np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "qFzc5GzDBhH-"
      },
      "outputs": [],
      "source": [
        "data.dropna(axis=0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "A0yFghFuBhFz",
        "outputId": "82c73f4e-2df2-4134-f400-698e13387ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Text  \\\n",
              "0                                                                                                                                                                                                                                                        I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.   \n",
              "1                                                                                                                                                                                                                                                                                                                                 Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".   \n",
              "2  This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.   \n",
              "3                                                                                                                                                                                                                                                                                                    If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                   Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.   \n",
              "\n",
              "                 Summary  \\\n",
              "0  Good Quality Dog Food   \n",
              "1      Not as Advertised   \n",
              "2  \"Delight\" says it all   \n",
              "3         Cough Medicine   \n",
              "4            Great taffy   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                 cleaned_text  \\\n",
              "0                                                                                                                        bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better   \n",
              "1                                                                                                                                                       product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo   \n",
              "2  confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch   \n",
              "3                                                                                                                                                                 looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal   \n",
              "4                                                                                                                                                                                                         great taffy great price wide assortment yummy taffy delivery quick taffy lover deal   \n",
              "\n",
              "         cleaned_summary  \n",
              "0  good quality dog food  \n",
              "1             advertised  \n",
              "2           delight says  \n",
              "3         cough medicine  \n",
              "4            great taffy  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f3746d8-2490-4856-8a59-c7833a4ae330\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Summary</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>cleaned_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better</td>\n",
              "      <td>good quality dog food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo</td>\n",
              "      <td>advertised</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch</td>\n",
              "      <td>delight says</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal</td>\n",
              "      <td>cough medicine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>great taffy great price wide assortment yummy taffy delivery quick taffy lover deal</td>\n",
              "      <td>great taffy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f3746d8-2490-4856-8a59-c7833a4ae330')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0f3746d8-2490-4856-8a59-c7833a4ae330 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0f3746d8-2490-4856-8a59-c7833a4ae330');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7047ae12-ad20-4537-8784-e90be2d7bf8f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7047ae12-ad20-4537-8784-e90be2d7bf8f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7047ae12-ad20-4537-8784-e90be2d7bf8f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 9440,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9440,\n        \"samples\": [\n          \"These are among the best chips I have ever eaten!  I first came upon them when I visited a CostPlus World Market store in Opelika, AL, in the Tiger Town Mall, in 2007.  I wasn't a big fan of dijon-flavored anything, but I decided to give these chips a try.  I was hooked.  I probably ended up buying over a dozen bags within just a few months' time.  I searched all over my part of Alabama (the northwestern part of the state) and could not find them.  I later tried to find them at the World Market in Hoover, at the Patton Creek Shopping Center, and they did not carry this flavor.  However, when I learned that they were available on Amazon.com, and also available for PRIME Shipping - I bought them as soon as I read \\\"Kettle Chips Honey Dijon, 9-Ounce Bags (Pack of 12)!\\\"<br /><br />You get a pretty big box, filled with 12 bags of these delicious morsels.  They last quite a while, but I will re-order, very, very soon!  I suggest you buy them, in this bulk form, and enjoy them - you won't be disappointed!  They go great with hamburgers and hot dogs - so they're perfect for spring and this coming summer's outdoor activities and cook-outs!<br /><br />Take my word for it, these chips are a hidden gem lost in the world of snack foods!  Buy some today, and fall in love like me!\",\n          \"Having recently purchased a single cup coffee maker, we discovered Green Mountain and their delicious varieties of coffee.  I love the Island Coconut flavor so much it is now a staple in our household with all other flavors taking a 2nd seat.  The roast of the coffee is wonderful and really lets the coconut shine through.  Try it with a little Chocolate Coffee Mate Creamer and you'll swear you were having a decadent treat straight from the Islands...  YUM!!!\",\n          \"hi every one I just wanted to share some of my experience with you about this product. First off all it taste pretty good but its kinda sugary and salty. Its an ok drink but when you think about it and you compare it to others this drink is not as healthy as others. Its more like a juice drink than a healthy one. Compared to Vitacoco it has more calories, twice the sodium, half the potassium and almost half the vitamin C. This drink also has less magnesium, phosphorous, and 4 times less calcium. The only this the drink does have over Vitacoco is the 1 less gram of sugar. Overall pay the extra price and get a better drink for your body and if you dont believe me go to the websites and check the facts below or check it out on their web site.<br /><br />O.N.E. COCONUT WATER WITH SPLASH OF PINEAPPLE<br />Calories: 75<br />Fat: 0<br />Cholesterol: 0<br />Sodium(115mg): 4.8%<br />potassium(299mg): 8.5%<br />Carbs(17g): 6%<br />Fiber: 1.2%<br />Sugar: 16g<br />protein: 0g<br /><br />vitamin c: 172%<br />calcium: 1.3%<br />magnesium: 0%<br />phosphorous: 0%<br />Iron: 1.8%<br /><br />COMPARED TO<br /><br />VITACOCO WITH SPLASH OF COCONUT<br />Calories: 80<br />Fat: 0<br />Cholesterol: 0<br />Sodium(40mg): 2%<br />potassium(740mg): 19%<br />Carbs(21g): 7%<br />Fiber: 0%<br />Sugar: 21g<br />protein: 0g<br /><br />vitamin c: 240%<br />calcium: 5%<br />magnesium: 10%<br />phosphorous: 5%<br />Iron: 0%\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8459,\n        \"samples\": [\n          \"I love snacks\",\n          \"Good brew but...\",\n          \"Made in Michigan since 1866\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9435,\n        \"samples\": [\n          \"someone suffers occasionally digestive difficulties kavli crispy thins among things eat times always keep crackers around add soups however also times thing properly digest great item around digestive difficulties associated flu stress work better bananas rice applesauce toast brat diet used relieving ibs thin easy chew crackers really great product\",\n          \"flavor combination ketchup chips good however coating applied heavily chips gritty thickly encrusted flavoring grit coated lips mouth grit crumbled fell clothes messy unpleasant needs lighter flavor coating\",\n          \"less week kitties grass much surprised reaction seemed like given previous home rushed right soon saw gnawed nothing first time got eaten plants sprang back life grew faster annihilated second time grass kind dried died okay plenty seeds pack\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7205,\n        \"samples\": [\n          \"needed\",\n          \"low calorie tasty snack\",\n          \"good coffee amazon com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "-QyCMMRt9gEq"
      },
      "outputs": [],
      "source": [
        "# Convert Text column from object to string\n",
        "#data['cleaned_text'] = data['cleaned_text'].astype('|S')\n",
        "data['cleaned_text'] = data['cleaned_text'].astype(object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "8I0KB-KF9WPu",
        "outputId": "00635e79-382d-4d5b-d32b-6d1acb421e8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 9440 entries, 0 to 9999\n",
            "Data columns (total 4 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Text             9440 non-null   object\n",
            " 1   Summary          9440 non-null   object\n",
            " 2   cleaned_text     9440 non-null   object\n",
            " 3   cleaned_summary  9440 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 368.8+ KB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "zgxpyX6gI3UZ"
      },
      "outputs": [],
      "source": [
        "data.to_csv('cleaned_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "PC28L2FdJAac"
      },
      "outputs": [],
      "source": [
        "#files.download('cleaned_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "poCiMRg2BhCo",
        "outputId": "d0299b9a-b138-437b-9134-f4e9c8db7e04"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Histogram for the summary length\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data['cleaned_summary'].str.len(), bins=50)\n",
        "plt.title('Summary Length Distribution')\n",
        "plt.xlabel('Summary Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "ORCL8oq0Bg3y",
        "outputId": "a84483b0-78ad-4046-a891-db1911388377"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Histogram for the text length\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data['cleaned_text'].str.len(), bins=50)\n",
        "plt.title('Text Length Distribution')\n",
        "plt.xlabel('Text Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOiIExEnIlMY"
      },
      "source": [
        "Most of the reviews are below 200 words, with majority of reviews being less than even 100 words. Therefore, for ease of computation, I will set the maximum word length for text and summary to 100 and 10 words each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "HPLL2UREBgye"
      },
      "outputs": [],
      "source": [
        "max_len_text=100\n",
        "max_len_summary=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "gYlZhnrqBgvZ"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(data['cleaned_text'],data['cleaned_summary'],test_size=0.1, shuffle=True, random_state=111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88G_6zpJkcs8",
        "outputId": "f9552251-d8cf-4040-b335-26e2292f9112"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2355                                                                                                                                                                                                                                                           bought change green split peas tasty like buy always like bob red mill products\n",
              "799                                                                                                                                                                                                                                           chocolate treat great gift arrived right expected gave girlfriend surprise gift absolutely loved\n",
              "7915    use whey soy protein alternative allergic soy sensitivity whey product seems free actually used shake use make protein pancakes find add extra cocoa powder make chocolatey stevia make sweet first came really awful pancakes found perfect ingredients doctor come amazing looking alternative whey soy definitely think good choice\n",
              "7804                                        vegetarian year relatively new meat substitute market far best veggie burger mix tried far better frozen patties get grocery stores added bonus versatility mix use many different things instead stuck patty form like frozen variety meat eating girlfriend really likes well tends pretty picky\n",
              "5282                                                                                                                                                                good bars gluten free favorite bar everything carry still good keep around hunger attack really want try something great try coconut cashew trubar incredible like naughty\n",
              "                                                                                                                                                                         ...                                                                                                                                                                  \n",
              "7573      photo shows potato carrot pieces butternut squash base upon opening potato carrot pieces found unlike cans bought local grocery store guess must change wolfgang puck recipe photo represent contents anymore several cans twelve pack delivered dented although none seriously packing better filling could prevented dents transit\n",
              "8974                                                                                                                                                                                                                                                          labs known picky eaters wow zuke top anything please one list toxic stuff either\n",
              "7526                                                                                                                                                                                                                                      recieved tea quickly helpful detailed brewing instructions tea absolutely delicious like dessert cup\n",
              "4218                                                                  daughter years discovered main ingredient rice flour usually good usually grainy case product makes fantastic pancakes like biscuits make used make corn dogs entire family ate one even noticed everyone loved amazon since taken subscribe save mad hope put back soon\n",
              "4862                                                                                                                                                                             bought big pack gum flavor last long found would usually pop new piece mouth minutes get another shot flavor would really recommend gum horrible great either\n",
              "Name: cleaned_text, Length: 8496, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L5go3HykfXt",
        "outputId": "c10a277f-2a35-4c8f-ea8e-5bcfa48e012d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6273                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          sooo happy amazon sells one grocery store neighborhood closing delicious perfect fiesta night\n",
              "5739                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  running vanilla would made using vanilla beans vodka price far cheaper getting locally figured worth shot sure thinking half pound vanilla beans works something like hard picture vanilla beans would good order halves someone quarters aroma incredible couple months since originally opened pantry still smells like vanilla even though wrapped two ziploc bags vanilla fantastic never tried vanilla produced absolutely missing distinct definite difference homemade buy store favor hit add cart check glad\n",
              "3238                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          husband love hot chocolate auto shipment every month going change every month\n",
              "6201                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         buying coffee least years love smooth taste hint hazelnut friends enjoy coming good cup coffee\n",
              "8325                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      wolfgang puck coffee chef reseerve columbian decaf fantastic delicious definitely recommend product anyone wants great cup coffee\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
              "6128                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         tastes delisious works efectively drink rigth time lean body suplement best flavor ever tasted\n",
              "9991    good product high quality majority people post amazon medical providers least bit inclined area sciences infant nutrition large amount misinformation misinformation evident propagated throughout reviews formula product specifically top echelon quality regard consistency nutritional value great success product pleased overall adjunct breastmilk abbott labs large research reagent laboratory medical sciences rigorous quality controls place published research also provide reagents scientific research community said would less inclined trust small makers formula said done components similac products highest quality monitored advanced scientific protocols well researched published studies would trust brand similac smaller names plain simple keep mind best suits baby nascent developing tracts babies experience degree gas discomfort regardless formula nutritional source sad see people reviews propagating misinformation neither pediatricians physicians research best avoid ulterior motives well misinformative reviews internet blogs overly slanted biased\n",
              "7873                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               chips family favorite absolutely fabulous guacamole salty pick salsa well especially fresh salsa shape get right amount salsa chips take dip compliment whatever dip fine without dip also used salty chip might find bit plain tortilla chips staple house best get except authentic mexican restaurant hope enjoy much\n",
              "3717                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               hot chocolate cups work like coffe cups coffee cups screen inside cup keep coffee grounds cup also makes last drop liquid fall cup much coffee first drip cafe escape hot cocoa cups screen chocolate powder empties cup fast last amount water come keurig mostly water sits top cup stir drinking otherwise taste sweetener also cups made serving size use appropriate cup size taste\n",
              "8955                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 lamb jerky soft treats first product tried zuke big hit continue one little schnoodle favorite treats soft jerky squares easily split pieces use training play sessions seems especially love reward zuke soft lamb jerky although performs eagerly reward zuke ounce mini bakes dog treats peanut butter blueberryz zuke mini bakes chicken cherry guess stickin zuke\n",
              "Name: cleaned_text, Length: 944, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "53dZJdyFBgin"
      },
      "outputs": [],
      "source": [
        "# tokenize\n",
        "src_tokens = tokenize(x_train)\n",
        "tgt_tokens = tokenize(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Sl9yvx8wKQa3"
      },
      "outputs": [],
      "source": [
        "# build vocabulary on dataset\n",
        "src_vocab = Vocab(src_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
        "tgt_vocab = Vocab(tgt_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "IT9lZ1CvMjbP"
      },
      "outputs": [],
      "source": [
        "src_array, src_valid_len = build_array_sum(src_tokens, src_vocab, max_len_text)\n",
        "tgt_array, tgt_valid_len = build_array_sum(tgt_tokens, tgt_vocab, max_len_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "17rNHg18Mr6y"
      },
      "outputs": [],
      "source": [
        "data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Vk622eE8MvL8"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "data_iter = load_array(data_arrays, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "4BgSvKj7NCmX"
      },
      "outputs": [],
      "source": [
        "num_hiddens, num_layers, dropout, num_steps = 32, 2, 0.1, 10\n",
        "ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
        "key_size, query_size, value_size = 32, 32, 32\n",
        "norm_shape = [32]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "-b0UfSxaNChB"
      },
      "outputs": [],
      "source": [
        "encoder = TransformerEncoder(len(src_vocab), key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,num_layers, dropout)\n",
        "decoder = TransformerDecoder(len(tgt_vocab), key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,num_layers, dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "dNEpXlAbNCeA"
      },
      "outputs": [],
      "source": [
        "net = Transformer(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "WAMyx0U7NCbJ"
      },
      "outputs": [],
      "source": [
        "#nn.init.xavier_uniform_(net.encoder.embed_tokens.weight)\n",
        "#nn.init.xavier_uniform_(net.encoder.weight)\n",
        "#nn.init.xavier_uniform_(net.weight) # for initialising the weights of the fully connected layers in the model\n",
        "#nn.init.xavier_uniform_(weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eSrKg0OVlsT",
        "outputId": "d54040ee-80ae-4799-e8bf-097c7f9c082f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): TransformerEncoder(\n",
              "    (embedding): Embedding(16822, 32)\n",
              "    (pos_encoding): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (blks): Sequential(\n",
              "      (block0): EncoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm1): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (ffn): PositionWiseFFN(\n",
              "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
              "        )\n",
              "        (addnorm2): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (block1): EncoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm1): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (ffn): PositionWiseFFN(\n",
              "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
              "        )\n",
              "        (addnorm2): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): TransformerDecoder(\n",
              "    (embedding): Embedding(4014, 32)\n",
              "    (pos_encoding): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (blks): Sequential(\n",
              "      (block0): DecoderBlock(\n",
              "        (attention1): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm1): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (attention2): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm2): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (ffn): PositionWiseFFN(\n",
              "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
              "        )\n",
              "        (addnorm3): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (block1): DecoderBlock(\n",
              "        (attention1): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm1): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (attention2): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm2): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (ffn): PositionWiseFFN(\n",
              "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
              "        )\n",
              "        (addnorm3): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (dense): Linear(in_features=32, out_features=4014, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "net.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "OxsQDp0BNCX_"
      },
      "outputs": [],
      "source": [
        "lr = 0.005\n",
        "num_epochs = 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1235Ep9qWY9W",
        "outputId": "c593af8e-1c39-4e68-ad0d-c6bffc4439c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLDiUBhPVw9p",
        "outputId": "bbaa97ac-b9ef-4024-b0f6-393f8a2db37c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP8ilPOENCVr",
        "outputId": "b2f66230-4cd2-48ed-9396-8ee2b906858c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done with epoch number: 1\n",
            "Done with epoch number: 2\n",
            "Done with epoch number: 3\n",
            "Done with epoch number: 4\n",
            "Done with epoch number: 5\n",
            "Done with epoch number: 6\n",
            "Done with epoch number: 7\n",
            "Done with epoch number: 8\n",
            "Done with epoch number: 9\n",
            "Done with epoch number: 10\n",
            "Done with epoch number: 11\n",
            "Done with epoch number: 12\n",
            "Done with epoch number: 13\n",
            "Done with epoch number: 14\n",
            "Done with epoch number: 15\n",
            "Done with epoch number: 16\n",
            "Done with epoch number: 17\n",
            "Done with epoch number: 18\n",
            "Done with epoch number: 19\n",
            "Done with epoch number: 20\n",
            "Done with epoch number: 21\n",
            "Done with epoch number: 22\n",
            "Done with epoch number: 23\n",
            "Done with epoch number: 24\n",
            "Done with epoch number: 25\n",
            "Done with epoch number: 26\n",
            "Done with epoch number: 27\n",
            "Done with epoch number: 28\n",
            "Done with epoch number: 29\n",
            "Done with epoch number: 30\n",
            "Done with epoch number: 31\n",
            "Done with epoch number: 32\n",
            "Done with epoch number: 33\n",
            "Done with epoch number: 34\n",
            "Done with epoch number: 35\n",
            "Done with epoch number: 36\n",
            "Done with epoch number: 37\n",
            "Done with epoch number: 38\n",
            "Done with epoch number: 39\n",
            "Done with epoch number: 40\n",
            "Done with epoch number: 41\n",
            "Done with epoch number: 42\n",
            "Done with epoch number: 43\n",
            "Done with epoch number: 44\n",
            "Done with epoch number: 45\n",
            "Done with epoch number: 46\n",
            "Done with epoch number: 47\n",
            "Done with epoch number: 48\n",
            "Done with epoch number: 49\n",
            "Done with epoch number: 50\n",
            "Done with epoch number: 51\n",
            "Done with epoch number: 52\n",
            "Done with epoch number: 53\n",
            "Done with epoch number: 54\n",
            "Done with epoch number: 55\n",
            "Done with epoch number: 56\n",
            "Done with epoch number: 57\n",
            "Done with epoch number: 58\n",
            "Done with epoch number: 59\n",
            "Done with epoch number: 60\n",
            "Done with epoch number: 61\n",
            "Done with epoch number: 62\n",
            "Done with epoch number: 63\n",
            "Done with epoch number: 64\n",
            "Done with epoch number: 65\n",
            "Done with epoch number: 66\n",
            "Done with epoch number: 67\n",
            "Done with epoch number: 68\n",
            "Done with epoch number: 69\n",
            "Done with epoch number: 70\n",
            "Done with epoch number: 71\n",
            "Done with epoch number: 72\n",
            "Done with epoch number: 73\n",
            "Done with epoch number: 74\n",
            "Done with epoch number: 75\n",
            "Done with epoch number: 76\n",
            "Done with epoch number: 77\n",
            "Done with epoch number: 78\n",
            "Done with epoch number: 79\n",
            "Done with epoch number: 80\n",
            "Done with epoch number: 81\n",
            "Done with epoch number: 82\n",
            "Done with epoch number: 83\n",
            "Done with epoch number: 84\n",
            "Done with epoch number: 85\n",
            "Done with epoch number: 86\n",
            "Done with epoch number: 87\n",
            "Done with epoch number: 88\n",
            "Done with epoch number: 89\n",
            "Done with epoch number: 90\n",
            "Done with epoch number: 91\n",
            "Done with epoch number: 92\n",
            "Done with epoch number: 93\n",
            "Done with epoch number: 94\n",
            "Done with epoch number: 95\n",
            "Done with epoch number: 96\n",
            "Done with epoch number: 97\n",
            "Done with epoch number: 98\n",
            "Done with epoch number: 99\n",
            "Done with epoch number: 100\n",
            "Done with epoch number: 101\n",
            "Done with epoch number: 102\n",
            "Done with epoch number: 103\n",
            "Done with epoch number: 104\n",
            "Done with epoch number: 105\n",
            "Done with epoch number: 106\n",
            "Done with epoch number: 107\n",
            "Done with epoch number: 108\n",
            "Done with epoch number: 109\n",
            "Done with epoch number: 110\n",
            "Done with epoch number: 111\n",
            "Done with epoch number: 112\n",
            "Done with epoch number: 113\n",
            "Done with epoch number: 114\n",
            "Done with epoch number: 115\n",
            "Done with epoch number: 116\n",
            "Done with epoch number: 117\n",
            "Done with epoch number: 118\n",
            "Done with epoch number: 119\n",
            "Done with epoch number: 120\n",
            "Done with epoch number: 121\n",
            "Done with epoch number: 122\n",
            "Done with epoch number: 123\n",
            "Done with epoch number: 124\n",
            "Done with epoch number: 125\n",
            "Done with epoch number: 126\n",
            "Done with epoch number: 127\n",
            "Done with epoch number: 128\n",
            "Done with epoch number: 129\n",
            "Done with epoch number: 130\n",
            "Done with epoch number: 131\n",
            "Done with epoch number: 132\n",
            "Done with epoch number: 133\n",
            "Done with epoch number: 134\n",
            "Done with epoch number: 135\n",
            "Done with epoch number: 136\n",
            "Done with epoch number: 137\n",
            "Done with epoch number: 138\n",
            "Done with epoch number: 139\n",
            "Done with epoch number: 140\n",
            "Done with epoch number: 141\n",
            "Done with epoch number: 142\n",
            "Done with epoch number: 143\n",
            "Done with epoch number: 144\n",
            "Done with epoch number: 145\n",
            "Done with epoch number: 146\n",
            "Done with epoch number: 147\n",
            "Done with epoch number: 148\n",
            "Done with epoch number: 149\n",
            "Done with epoch number: 150\n",
            "Done with epoch number: 151\n",
            "Done with epoch number: 152\n",
            "Done with epoch number: 153\n",
            "Done with epoch number: 154\n",
            "Done with epoch number: 155\n",
            "Done with epoch number: 156\n",
            "Done with epoch number: 157\n",
            "Done with epoch number: 158\n",
            "Done with epoch number: 159\n",
            "Done with epoch number: 160\n",
            "Done with epoch number: 161\n",
            "Done with epoch number: 162\n",
            "Done with epoch number: 163\n",
            "Done with epoch number: 164\n",
            "Done with epoch number: 165\n",
            "Done with epoch number: 166\n",
            "Done with epoch number: 167\n",
            "Done with epoch number: 168\n",
            "Done with epoch number: 169\n",
            "Done with epoch number: 170\n",
            "Done with epoch number: 171\n",
            "Done with epoch number: 172\n",
            "Done with epoch number: 173\n",
            "Done with epoch number: 174\n",
            "Done with epoch number: 175\n",
            "Done with epoch number: 176\n",
            "Done with epoch number: 177\n",
            "Done with epoch number: 178\n",
            "Done with epoch number: 179\n",
            "Done with epoch number: 180\n",
            "Done with epoch number: 181\n",
            "Done with epoch number: 182\n",
            "Done with epoch number: 183\n",
            "Done with epoch number: 184\n",
            "Done with epoch number: 185\n",
            "Done with epoch number: 186\n",
            "Done with epoch number: 187\n",
            "Done with epoch number: 188\n",
            "Done with epoch number: 189\n",
            "Done with epoch number: 190\n",
            "Done with epoch number: 191\n",
            "Done with epoch number: 192\n",
            "Done with epoch number: 193\n",
            "Done with epoch number: 194\n",
            "Done with epoch number: 195\n",
            "Done with epoch number: 196\n",
            "Done with epoch number: 197\n",
            "Done with epoch number: 198\n",
            "Done with epoch number: 199\n",
            "Done with epoch number: 200\n",
            "Done with epoch number: 201\n",
            "Done with epoch number: 202\n",
            "Done with epoch number: 203\n",
            "Done with epoch number: 204\n",
            "Done with epoch number: 205\n",
            "Done with epoch number: 206\n",
            "Done with epoch number: 207\n",
            "Done with epoch number: 208\n",
            "Done with epoch number: 209\n",
            "Done with epoch number: 210\n",
            "Done with epoch number: 211\n",
            "Done with epoch number: 212\n",
            "Done with epoch number: 213\n",
            "Done with epoch number: 214\n",
            "Done with epoch number: 215\n",
            "Done with epoch number: 216\n",
            "Done with epoch number: 217\n",
            "Done with epoch number: 218\n",
            "Done with epoch number: 219\n",
            "Done with epoch number: 220\n",
            "Done with epoch number: 221\n",
            "Done with epoch number: 222\n",
            "Done with epoch number: 223\n",
            "Done with epoch number: 224\n",
            "Done with epoch number: 225\n",
            "Done with epoch number: 226\n",
            "Done with epoch number: 227\n",
            "Done with epoch number: 228\n",
            "Done with epoch number: 229\n",
            "Done with epoch number: 230\n",
            "Done with epoch number: 231\n",
            "Done with epoch number: 232\n",
            "Done with epoch number: 233\n",
            "Done with epoch number: 234\n",
            "Done with epoch number: 235\n",
            "Done with epoch number: 236\n",
            "Done with epoch number: 237\n",
            "Done with epoch number: 238\n",
            "Done with epoch number: 239\n",
            "Done with epoch number: 240\n",
            "Done with epoch number: 241\n",
            "Done with epoch number: 242\n",
            "Done with epoch number: 243\n",
            "Done with epoch number: 244\n",
            "Done with epoch number: 245\n",
            "Done with epoch number: 246\n",
            "Done with epoch number: 247\n",
            "Done with epoch number: 248\n",
            "Done with epoch number: 249\n",
            "Done with epoch number: 250\n",
            "loss 0.202 on cuda:0\n"
          ]
        }
      ],
      "source": [
        "train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "l_OJtjC6NCSe"
      },
      "outputs": [],
      "source": [
        "sample_test = np.array(x_test[:10])\n",
        "actual_test = np.array(y_test[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE_vgl1HDRfz",
        "outputId": "0590f756-7205-4d6b-8dcb-fd362d55031d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "sample_test.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "sh9UDJ2PWh5p"
      },
      "outputs": [],
      "source": [
        "# Delete row number 24 from the sample_test and from the actual_test\n",
        "#sample_test = np.delete(sample_test, 24, axis= 0)\n",
        "#actual_test = np.delete(actual_test, 24, axis= 0)\n",
        "sample_test[4] = 'wolfgang puck coffee chef reserve columbian decaf fantastic delicious definitely recommend product anyone wants great cup coffee'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkQZ9xoWNCPg",
        "outputId": "19621ff9-6893-49fd-f38e-5936ffd0f12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. SAMPLE  : sooo happy amazon sells one grocery store neighborhood closing delicious perfect fiesta night.\n",
            "PREDICTED  : great product.\n",
            "ACTUAL     : amazingly delicious.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1. SAMPLE  : running vanilla would made using vanilla beans vodka price far cheaper getting locally figured worth shot sure thinking half pound vanilla beans works something like hard picture vanilla beans would good order halves someone quarters aroma incredible couple months since originally opened pantry still smells like vanilla even though wrapped two ziploc bags vanilla fantastic never tried vanilla produced absolutely missing distinct definite difference homemade buy store favor hit add cart check glad.\n",
            "PREDICTED  : great taste.\n",
            "ACTUAL     : excellent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "2. SAMPLE  : husband love hot chocolate auto shipment every month going change every month.\n",
            "PREDICTED  : great.\n",
            "ACTUAL     : love.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "3. SAMPLE  : buying coffee least years love smooth taste hint hazelnut friends enjoy coming good cup coffee.\n",
            "PREDICTED  : great coffee.\n",
            "ACTUAL     : great coffee.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "4. SAMPLE  : wolfgang puck coffee chef reserve columbian decaf fantastic delicious definitely recommend product anyone wants great cup coffee.\n",
            "PREDICTED  : great taste.\n",
            "ACTUAL     : wonderful cups.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "5. SAMPLE  : eat chips often favorites flavor texture traditional tortilla chips subtle lime flavoring makes special lime flavoring appears seasoning dusted chips seem barely flavor find looking still none quite enough tastes wish chips hint lime.\n",
            "PREDICTED  : yum.\n",
            "ACTUAL     : tasty.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "6. SAMPLE  : jelly tastes heavenly wont find anything good grocery store say ordering shortly.\n",
            "PREDICTED  : great taste.\n",
            "ACTUAL     : delicious.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "7. SAMPLE  : done side side comparison chips still taste like real deal snack sized bags well regular brand grocery store work portion control reduced fat yes please subscribe save much cheaper grocery store.\n",
            "PREDICTED  : great chips.\n",
            "ACTUAL     : delicious.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "8. SAMPLE  : purchased item cheaper olive juices looking good quality olive juice martinis pretty disappointed juice strong followed instructions put ratio bottle shaker made martini ratio part vodka part olive juice way salty olive juice powering made decent dirty martini parts vodka part olive juice would recommend item.\n",
            "PREDICTED  : great product.\n",
            "ACTUAL     : olive juice.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "9. SAMPLE  : recommended friend saw gym seen times fatted calf napa didnt really pay attention never buy beans bag grocery store huge difference taste quality sent son aspiring chef gourmet foodie box beans bag well.\n",
            "PREDICTED  : great product.\n",
            "ACTUAL     : outstanding.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "counter = 0\n",
        "for s, a in zip(sample_test, actual_test):\n",
        "    pred_sum, _ = predict_seq2seq(net, s, src_vocab, tgt_vocab, 10, device)\n",
        "    print(f\"{counter}. SAMPLE  : {s}.\")\n",
        "    print(\"PREDICTED  : {}.\".format(pred_sum), end='\\n')\n",
        "    print(\"ACTUAL     : {}.\".format(a))\n",
        "    counter += 1\n",
        "    print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H44b_jMGJ-W",
        "outputId": "d7ebce2f-22fc-405c-ef1c-db0fc9959be2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): TransformerEncoder(\n",
              "    (embedding): Embedding(16822, 32)\n",
              "    (pos_encoding): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (blks): Sequential(\n",
              "      (block0): EncoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm1): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (ffn): PositionWiseFFN(\n",
              "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
              "        )\n",
              "        (addnorm2): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (block1): EncoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm1): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (ffn): PositionWiseFFN(\n",
              "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
              "        )\n",
              "        (addnorm2): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): TransformerDecoder(\n",
              "    (embedding): Embedding(4014, 32)\n",
              "    (pos_encoding): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (blks): Sequential(\n",
              "      (block0): DecoderBlock(\n",
              "        (attention1): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm1): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (attention2): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm2): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (ffn): PositionWiseFFN(\n",
              "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
              "        )\n",
              "        (addnorm3): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (block1): DecoderBlock(\n",
              "        (attention1): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm1): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (attention2): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "        (addnorm2): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (ffn): PositionWiseFFN(\n",
              "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
              "        )\n",
              "        (addnorm3): AddNorm(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (dense): Linear(in_features=32, out_features=4014, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "net.eval()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}